{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe, Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col,trim,ltrim,rtrim,when,regexp_replace,concat_ws, lit, sha2\n",
    "\n",
    "import os\n",
    "import re \n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Iniciando o tratamento dos dados\n",
    "\n",
    "#===========================================CLIENTE===========================================\n",
    "#Criando DataFrame\n",
    "df_clientes = spark.sql(\"select * from desafio_curso.tbl_clientes\")\n",
    "#Convertendo os tipos de dados\n",
    "df_clientes = df_clientes.withColumn(\"address_number\",col(\"address_number\").cast(IntegerType()))\\\n",
    "        .withColumn(\"business_unit\",col(\"business_unit\").cast(IntegerType()))\\\n",
    "        .withColumn(\"customerkey\",col(\"customerkey\").cast(IntegerType()))\\\n",
    "        .withColumn(\"division\",col(\"division\").cast(IntegerType()))\\\n",
    "        .withColumn(\"region_code\",col(\"region_code\").cast(IntegerType()))\n",
    "#Tratando as colunas vazias\n",
    "df_clientes = df_clientes.withColumn('line_of_business', regexp_replace('line_of_business', '   ', 'Não Informado'))\n",
    "#Criando view\n",
    "df_clientes.createOrReplaceTempView('tb_clientes')\n",
    "\n",
    "#===========================================DIVISAO===========================================\n",
    "#Criando DataFrame\n",
    "df_divisao = spark.sql(\"select * from desafio_curso.tbl_divisao\")\n",
    "#Convertendo os tipos de dados\n",
    "df_divisao = df_divisao.withColumn(\"division\",col(\"division\").cast(IntegerType()))\n",
    "#Criando view\n",
    "df_divisao.createOrReplaceTempView('tb_divisao')\n",
    "\n",
    "#===========================================ENDEREÇO===========================================\n",
    "#Criando DataFrame\n",
    "df_endereco = spark.sql(\"select * from desafio_curso.tbl_endereco\")\n",
    "#Convertendo os tipos de dados\n",
    "df_endereco = df_endereco.withColumn(\"address_number\",col(\"address_number\").cast(IntegerType()))\n",
    "#Tratando as colunas vazias\n",
    "df_endereco = df_endereco.withColumn('city', regexp_replace('city', '                        ', 'Não Informado'))            .withColumn('customer_address_1', regexp_replace('customer_address_1', '                                       ', 'Não Informado'))            .withColumn('customer_address_2', regexp_replace('customer_address_2', '                                       ', 'Não Informado'))            .withColumn('customer_address_3', regexp_replace('customer_address_3', '                                       ', 'Não Informado'))            .withColumn('customer_address_4', regexp_replace('customer_address_4', '                                       ', 'Não Informado'))            .withColumn('zip_code', regexp_replace('zip_code', '            ', 'Não Informado'))\n",
    "df_endereco = df_endereco.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df_endereco.columns])\n",
    "df_endereco = df_endereco.na.fill(\"Não Informado\")\n",
    "#Criando view\n",
    "df_endereco.createOrReplaceTempView('tb_endereco')\n",
    "\n",
    "#===========================================REGIÃO===========================================\n",
    "#Criando DataFrame\n",
    "df_regiao = spark.sql(\"select * from desafio_curso.tbl_regiao\")\n",
    "#Convertendo os tipos de dados\n",
    "df_regiao = df_regiao.withColumn(\"region_code\",col(\"region_code\").cast(IntegerType()))\n",
    "#Criando view\n",
    "df_regiao.createOrReplaceTempView('tb_regiao')\n",
    "\n",
    "#===========================================VENDAS===========================================\n",
    "#Criando DataFrame\n",
    "df_vendas = spark.sql(\"select * from desafio_curso.tbl_vendas\")\n",
    "#Convertendo os tipos de dados\n",
    "df_vendas = df_vendas.withColumn(\"customerkey\",col(\"customerkey\").cast(IntegerType()))\\\n",
    "        .withColumn(\"discount_amount\",col(\"discount_amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"invoice_number\",col(\"invoice_number\").cast(IntegerType()))\\\n",
    "        .withColumn(\"item_number\",col(\"item_number\").cast(IntegerType()))\\\n",
    "        .withColumn(\"line_number\",col(\"item_number\").cast(IntegerType()))\\\n",
    "        .withColumn(\"list_price\",col(\"list_price\").cast(DoubleType()))\\\n",
    "        .withColumn(\"order_number\",col(\"order_number\").cast(IntegerType()))\\\n",
    "        .withColumn(\"sales_amount\",col(\"sales_amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"sales_amount_based_on_list_price\",col(\"sales_amount_based_on_list_price\").cast(DoubleType()))\\\n",
    "        .withColumn(\"sales_cost_amount\",col(\"sales_cost_amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"sales_margin_amount\",col(\"sales_margin_amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"sales_price\",col(\"sales_price\").cast(DoubleType()))\\\n",
    "        .withColumn(\"sales_quantity\",col(\"sales_quantity\").cast(IntegerType()))\\\n",
    "        .withColumn(\"sales_rep\",col(\"sales_rep\").cast(IntegerType()))\n",
    "#Convertendo string para timestamp\n",
    "df_vendas = df_vendas.select('discount_amount',\n",
    "                             'invoice_number',\n",
    "                             'item_class',\n",
    "                             'item_number',\n",
    "                             'item',\n",
    "                             'line_number',\n",
    "                             'list_price',\n",
    "                             'order_number',\n",
    "                             'sales_amount',\n",
    "                             'sales_amount_based_on_list_price',\n",
    "                             'sales_cost_amount',\n",
    "                             'sales_margin_amount',\n",
    "                             'sales_price',\n",
    "                             'sales_quantity',\n",
    "                             'sales_rep',\n",
    "                             'u_m', \n",
    "                             'customerkey',\n",
    "                             'dt_foto',\n",
    "                             from_unixtime(unix_timestamp('actual_delivery_date', 'dd/MM/yyy')).alias('actual_delivery_date'),\n",
    "                             from_unixtime(unix_timestamp('invoice_date', 'dd/MM/yyy')).alias('invoice_date'),\n",
    "                             from_unixtime(unix_timestamp('promised_delivery_date', 'dd/MM/yyy')).alias('promised_delivery_date'),\n",
    "                             from_unixtime(unix_timestamp('datekey', 'dd/MM/yyy')).alias('datekey')\n",
    "                            )\n",
    "#Tratando as colunas vazias\n",
    "df_vendas = df_vendas.na.fill(value=0)\n",
    "df_vendas = df_vendas.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df_vendas.columns])\n",
    "df_vendas = df_vendas.na.fill(\"Não Informado\")\n",
    "df_vendas = df_vendas.withColumn(\"datekey\",to_timestamp(col(\"datekey\")))\n",
    "df_vendas = df_vendas.withColumn(\"promised_delivery_date\",to_timestamp(col(\"promised_delivery_date\")))\n",
    "df_vendas = df_vendas.withColumn(\"invoice_date\",to_timestamp(col(\"invoice_date\")))\n",
    "df_vendas = df_vendas.withColumn(\"actual_delivery_date\",to_timestamp(col(\"actual_delivery_date\")))\n",
    "#Criando view\n",
    "df_vendas.createOrReplaceTempView('tb_vendas')\n",
    "\n",
    "#===========================================STAGE===========================================\n",
    "\n",
    "#Criando tabelão com todos os dados\n",
    "query='''\n",
    "SELECT    c.customerkey\n",
    "          ,c.customer\n",
    "          ,c.customer_type\n",
    "          ,c.business_family\n",
    "          ,c.business_unit\n",
    "          ,c.division\n",
    "          ,d.division_name\n",
    "          ,c.line_of_business\n",
    "          ,c.phone\n",
    "          ,c.region_code\n",
    "          ,r.region_name\n",
    "          ,c.regional_sales_mgr\n",
    "          ,c.search_type\n",
    "          ,v.datekey\n",
    "          ,v.actual_delivery_date\n",
    "          ,v.discount_amount\n",
    "          ,v.invoice_date\n",
    "          ,v.invoice_number\n",
    "          ,v.item_class\n",
    "          ,v.item_number\n",
    "          ,v.item\n",
    "          ,v.line_number\n",
    "          ,v.list_price\n",
    "          ,v.order_number\n",
    "          ,v.promised_delivery_date\n",
    "          ,v.sales_amount\n",
    "          ,v.sales_amount_based_on_list_price\n",
    "          ,v.sales_cost_amount\n",
    "          ,v.sales_margin_amount\n",
    "          ,v.sales_price\n",
    "          ,v.sales_quantity\n",
    "          ,v.sales_rep\n",
    "          ,v.u_m\n",
    "          ,e.address_number\n",
    "          ,e.city\n",
    "          ,e.country\n",
    "          ,e.customer_address_1\n",
    "          ,e.customer_address_2\n",
    "          ,e.customer_address_3\n",
    "          ,e.customer_address_4\n",
    "          ,e.state\n",
    "          ,e.zip_code\n",
    "          ,e.dt_foto\n",
    "FROM      tb_vendas v\n",
    "          INNER JOIN tb_clientes c ON v.customerkey == c.customerkey\n",
    "          INNER JOIN tb_regiao r ON c.region_code == r.region_code\n",
    "          INNER JOIN tb_divisao d ON c.division == d.division\n",
    "          LEFT JOIN tb_endereco e ON c.address_number == e.address_number\n",
    "'''\n",
    "\n",
    "#Criando DataFrame\n",
    "df_stage = spark.sql(query)\n",
    "#Adicionando as colunas do tempo\n",
    "df_stage = (df_stage\n",
    "            .withColumn('Ano', year(df_stage.invoice_date))\n",
    "            .withColumn('Mes', month(df_stage.invoice_date))\n",
    "            .withColumn('Dia', dayofmonth(df_stage.invoice_date))\n",
    "            .withColumn('Trimestre', quarter(df_stage.invoice_date))\n",
    "           )\n",
    "#Tratando as colunas nulas após os JOINS\n",
    "df_stage = df_stage.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df_stage.columns])\n",
    "df_stage = df_stage.na.fill(\"Não Informado\")\n",
    "\n",
    "#Gerando keys para as DIMs\n",
    "df_stage = df_stage.withColumn('key_cliente',sha2(col(\"customerkey\").cast(StringType()),256))\n",
    "df_stage = df_stage.withColumn('key_tempo',sha2(concat_ws('|', col('invoice_date'), col('Ano'),col('Mes'),col('Dia')),256))\n",
    "df_stage = df_stage.withColumn('key_localidade',sha2(concat_ws('|', col('division'), col('region_code'),col('address_number')),256))\n",
    "\n",
    "#Gerando VIEW\n",
    "df_stage.createOrReplaceTempView('tb_stage')\n",
    "\n",
    "#===========================================DIMENSÕES===========================================\n",
    "\n",
    "#Criando DataFrame para a dimensão de tempo\n",
    "dim_clientes = spark.sql('''\n",
    "    SELECT DISTINCT key_cliente\n",
    "        ,business_family\n",
    "        ,customer \n",
    "        ,customer_type \n",
    "        ,line_of_business\n",
    "        ,regional_sales_mgr\n",
    "        ,search_type\n",
    "    FROM tb_stage    \n",
    "''')\n",
    "\n",
    "#Criando DataFrame para a dimensão de tempo\n",
    "dim_tempo = spark.sql('''\n",
    "    SELECT DISTINCT key_tempo\n",
    "        ,invoice_date\n",
    "        ,Ano \n",
    "        ,Mes \n",
    "        ,Dia\n",
    "        ,Trimestre\n",
    "    FROM tb_stage    \n",
    "''')\n",
    "\n",
    "dim_tempo = dim_tempo.withColumn('invoice_date',to_date('invoice_date'))\n",
    "\n",
    "#Criando DataFrame para a dimensão de localidade\n",
    "dim_localidade = spark.sql('''\n",
    "    SELECT DISTINCT key_localidade\n",
    "        ,division_name\n",
    "        ,region_name \n",
    "        ,country \n",
    "        ,state\n",
    "        ,city\n",
    "        ,zip_code\n",
    "    FROM tb_stage    \n",
    "''')\n",
    "\n",
    "#===========================================FATO===========================================\n",
    "\n",
    "#Criando o DataFrame da fato\n",
    "ft_vendas = spark.sql('''\n",
    "    SELECT DISTINCT key_cliente\n",
    "        ,key_tempo\n",
    "        ,key_localidade\n",
    "        ,count(distinct invoice_number) qty_vendas\n",
    "        ,sum(sales_quantity) quantity\n",
    "        ,sum(sales_amount) amount\n",
    "        ,sum(sales_cost_amount) cost\n",
    "        ,sum(sales_amount - sales_cost_amount) total_amount\n",
    "    FROM tb_stage    \n",
    "    GROUP BY key_cliente\n",
    "        ,key_tempo\n",
    "        ,key_localidade\n",
    "''')\n",
    "\n",
    "#===========================================CSV===========================================\n",
    "\n",
    "#Procedimento para gerar os arquivos CSVs\n",
    "def criar_csv (df,name):\n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format('csv')\\\n",
    "        .option('header',True)\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('sep',';')\\\n",
    "        .save(\"/datalake/gold/\"+name)\n",
    "    \n",
    "    copiar = \"hdfs dfs -get /datalake/gold/\"+name+\"/*.csv /input/gold/\"+name+\".csv\"\n",
    "    \n",
    "    os.system(copiar)\n",
    "\n",
    "#Criando os arquivos csv    \n",
    "criar_csv(dim_tempo,'dim_tempo')\n",
    "criar_csv(dim_localidade,'dim_localidade')\n",
    "criar_csv(dim_clientes,'dim_clientes')\n",
    "criar_csv(ft_vendas,'ft_vendas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
